# Design of an Efficient TTFS-SNN-Based EEG Emotion Recognition System on Lightweight FPGA
基于 TTFS-SNN 的高效 EEG 情绪识别系统轻量 FPGA 实现
## 摘要
**关键词：** Spiking Neural Network (SNN)；Depthwise Separable Convolution；Energy-Efficient；Lightweight

## 1. INTRODUCTION

情绪调节对人类的认知、决策与社会行为的影响至关重要，是理解心理状态与构建智能系统的关键。随着情感计算和人机交互的发展，情绪识别（Emotion Recognition）被视为连接生理信号与情绪体验的重要桥梁[1]。现实应用中，情绪具有持续、缓变和个体依赖的特征，要求监测系统能够在长期、低侵入性和能效受限的条件下稳定运行[2]。

EEG（脑电图）作为一种反映大脑神经活动的生理信号，具有毫秒级的时间分辨率和高时效性，成为情绪监测的理想选择。与心率、皮肤电或呼吸等外周信号不同，EEG能够记录情绪加工的中枢神经过程，不依赖外部行为表现[3-4]。EEG信号天然展现出高度的动态性与事件驱动特性，这使其能敏锐捕捉情绪因内外刺激而产生的即时反应与连续演变。同时，大脑功能活动在神经表征上的稀疏性特质，也为从复杂信号中高效解码特定情绪状态提供了理论基础[5]。

其毫秒级时间分辨率使系统能够捕捉情绪状态的瞬时波动，从而支持连续、细粒度的动态监测。同时，EEG传感器的可穿戴技术与柔性电极材料发展[6,7]显著降低了使用门槛，使低侵入、长时程采集成为可能。这些特性使EEG成为构建能效最优、可长期运行的情绪监测系统的理想信号来源。

在实际应用中，情绪识别系统的部署不仅需要处理EEG信号的复杂性，还必须考虑到边缘设备上计算能力和能效的限制。在资源受限的边缘设备上，实时推理、低功耗以及高效计算成为实现情绪识别系统的核心挑战[8]。传统的计算平台如CPU/GPU，尽管在高计算需求下表现优秀，但其能耗高、计算密集，难以在能效受限场景保持高效运算[9, 10]； MCU 虽然具备较低的功耗，但其资源配置是固定的，计算能力和存储资源在设计时就已经确定。即使在某些计算任务中不使用所有的硬件模块，未启用的模块仍然存在并消耗功耗，这导致了额外的静态功耗。此外，MCU的并行算力有限，难以满足深度模型的复杂计算和低延迟需求[11, 12]。相对而言，现场可编程门阵列（FPGA）凭借其可重构并行架构和细粒度的数据流控制，提供了在边缘设备上加速计算的理想平台[13]。FPGA的硬件资源是可重构的，可以根据任务的需求动态配置，关闭不必要的模块，从而显著降低能耗。通过并行化与流水线化设计，energy-per-inference显著更优，并在延迟方面具有天然优势[14]。同时，其设计理念与可移植性使其成为面向未来类脑 ASIC 架构的理想过渡平台。传统的 FPGA 多采用同步电路设计，其所有运算单元依赖统一的全局时钟信号进行触发与同步，但全局时钟的持续分配与切换会带来显著的动态功耗开销，同时在处理稀疏或事件驱动型信号时，仍需执行大量冗余运算，导致能效利用率较低。近年来，部分研究[15]在 FPGA 上探索了异步电路的实现，通过事件驱动机制取代全局时钟信号，以实现按需触发的计算过程。该设计不仅显著降低了时钟网络的功耗，更契合生物神经系统的事件触发特性。此外，这种异步、事件驱动的计算方式使 FPGA 能够更高效地应对 EEG 信号中稀疏且动态变化的计算任务。

尽管 FPGA 为低功耗并行计算提供了理想的硬件载体，但要真正发挥其异步与事件驱动架构的优势，前提是运行其上的计算模型本身需具备事件触发或稀疏计算特性。当前主流的深度学习模型，如 V. J. Lawhern 等提出的 EEGNet [16]，在脑电特征提取中表现优异，但其结构仍依赖密集的乘加运算与层间同步机制，包括卷积、批归一化与时间步展开等操作，导致计算与存储开销较高，难以与异步硬件的按需计算模式相匹配。同样地，基于循环单元的时间序列模型，如用于情感识别的 LSTM [17]，依赖严格的时序更新与全局同步处理，难以在无全局时钟的事件驱动平台上实现高效并行化。此外，基于图卷积的 MSFR-GCN [18] 虽然在空间拓扑关系建模方面具有优势，但其对邻接矩阵运算与不规则内存访问的依赖进一步增加了 FPGA 上的实现复杂度与延迟开销。因此，尽管模型压缩、剪枝与量化等轻量化策略在一定程度上可降低运算负担，但在兼顾实时性与能效的条件下，传统的同步深度学习范式仍难以充分释放 FPGA 异步、事件驱动体系的潜力。

因此，解决这一问题的关键不仅在于算法优化，而在于计算范式的根本转变。作为类脑计算的核心，脉冲神经网络（SNN）凭借其事件驱动机制和稀疏激活特性[19]，能够显著减少计算资源的消耗，同时与EEG信号的动态特性高度契合。通过仅在信号发生变化时激活神经元，SNN为实现低功耗、高效的实时推理提供了潜力，使其成为边缘设备上情绪识别系统的理想选择[20]。已有研究验证了 SNN 在情绪识别与压力检测任务中的可行性。例如，Kumar 等[21]在类脑硬件上实现了 EEG 信号解码，展示了低功耗情绪识别的潜力；Doborjeh 等[22]则利用递归式 SNN 有效建模了情绪动态过程。这些成果表明，类脑计算有望成为实现连续、实时、低功耗情绪识别的关键技术路径。

令人鼓舞的是，研究者已开始针对这一挑战展开探索，并提出多种方案以突破性能瓶颈。近年来，基于脉冲神经网络的轻量化模型逐渐展现出在资源受限设备上的应用潜力[23,24]。然而，尽管这些模型在事件驱动计算与能效优化方面取得了显著进展，其在 FPGA 平台上的实现尚未广泛应用于情绪识别领域,也尚未与异步电路设计相结合以进一步释放事件驱动体系的能效潜力。换言之，如何将 SNN 的事件驱动机制与 FPGA 的可重构特性相结合，并与异步电路架构协同优化，以实现对 EEG 情绪信号的高效、低功耗处理，仍是当前研究的关键难题。

针对上述问题，本研究提出了一种创新的系统架构，融合 FPGA 的可重构硬件特性与 SNN 的事件驱动计算机制，构建兼具低功耗、高并行性与可扩展性的 EEG 情绪识别系统。具体的贡献如下：

1.面向EEG情绪识别提出了一种基于FPGA的轻量化系统，energy-per-inference低至并且保持了高精度和高实时性的优点。

2.本研究提出了一种面向情绪识别、可在FPGA上落地的动态自适应脉冲计算框架（DA-SNN），通过轻量化特征提取（DSGM）、无除法TTFS稀疏编码和在线时间窗调整机制（ATSNN），实现了从特征域到时间域的全链路能效优化,达到了算法级稀疏性、硬件级友好性与时序级可塑性的统一。

3.在硬件层面构建了GALS（Globally Asynchronous, Locally Synchronous）架构，通过事件驱动的稀疏触发与模块级并行调度显著降低了时钟功耗并提升系统吞吐率。

[1] K. J. Wang, R. Vinjamuri, M. Alimardani, *et al.*, “NeuroDesign in Human-Robot Interaction: The making of engaging HRI technology your brain can't resist,” *Frontiers in Robotics and AI*, vol. 12, Art. no. 1699371, 2025.

[2] R. Pillalamarri and U. Shanmugam, “A review on EEG-based multimodal learning for emotion recognition,” *Artif. Intell. Rev.*, vol. 58, no. 5, p. 131, 2025.

[3] X. Li, D. Song, P. Zhang, et al., “Exploring EEG features in cross-subject emotion recognition,” *Front. Neurosci.*, vol. 12, p. 162, 2018.

[4] A. Y. Begum, E. E. Lawrence, M. Saravanan, et al., “Advanced biosignal processing and emotion recognition through artificial intelligence,” in *Applied Mathematical Modeling for Biomedical Robotics and Wearable Devices*. Academic Press, 2026, pp. 59–78.

[5] X. Li, Y. Zhang, P. Tiwari, et al., “EEG based emotion recognition: A tutorial and review,” *ACM Comput. Surv.*, vol. 55, no. 4, pp. 1–57, 2022.

[6] F. Wang, Y. Xue, X. Chen, and J. Li, “3D printed implantable hydrogel bioelectronics for electrophysiological monitoring and electrical modulation,” *Advanced Functional Materials*, vol. 34, no. 21, p. 2314471, 2024.

[7] K. Zheng, C. Zheng, L. Zhu, and Y. Liu, “Machine learning enabled reusable adhesion, entangled network-based hydrogel for long-term, high-fidelity EEG recording and attention assessment,” *Nano-Micro Letters*, vol. 17, no. 1, p. 281, 2025.

[8] Z. Aalam, S. Aziz, K. L. Lew, et al., “Real-Time Emotion Detection Using Artificial Intelligence: A Review,” Int. J. Robot. Autom. Sci., vol. 7, no. 1, pp. 104–110, 2025.

[9] S. Mittal, “A survey of techniques for approximate computing,” ACM Comput. Surv. (CSUR), vol. 48, no. 4, pp. 1–33, 2016.

[10] A. Reuther, P. Michaleas, M. Jones, et al., “Survey of machine learning accelerators,” in Proc. 2020 IEEE High Performance Extreme Computing Conf. (HPEC), 2020, pp. 1–12.

[11] J. Lin, W.-M. Chen, Y. Lin, et al., “MCUNet: Tiny deep learning on IoT devices,” in *Adv. Neural Inf. Process. Syst.*, vol. 33, 2020, pp. 11711–11722.

[12] R. Sanchez-Iborra and A. F. Skarmeta, “TinyML-enabled frugal smart objects: Challenges and opportunities,” *IEEE Circuits Syst. Mag.*, vol. 20, no. 3, pp. 4–18, 2020.

[13] V. Sze, Y.-H. Chen, T.-J. Yang, et al., “Efficient processing of deep neural networks: A tutorial and survey,” *Proc. IEEE*, vol. 105, no. 12, pp. 2295–2329, 2017.

[14] K. Guo, S. Zeng, J. Yu, et al., “A survey of FPGA-based neural network inference accelerators,” *ACM Trans. Reconfig. Technol. Syst. (TRETS)*, vol. 12, no. 1, pp. 1–26, 2019.

[15] X. Zhang, J. Zhang, H. Huang, et al., “An Asynchronous RISC-V-based SNN Processor with Custom ISA Extensions for Programmable On-Chip Learning,” in *Proc. 29th IEEE Int. Symp. Asynchronous Circuits Syst. (ASYNC)*, IEEE, 2025, pp. 1–8.

[16] V. J. Lawhern, A. J. Solon, N. R. Waytowich, et al., “EEGNet: a compact convolutional neural network for EEG-based brain–computer interfaces,” *J. Neural Eng.*, vol. 15, no. 5, p. 056013, 2018.

[17] S. Alhagry, A. A. Fahmy and R. A. El-Khoribi, “Emotion recognition based on EEG using LSTM recurrent neural network,” *Int. J. Adv. Comput. Sci. Appl.*, vol. 8, no. 10, 2017.

[18] D. Pan, H. Zheng, F. Xu, et al., “MSFR-GCN: A multi-scale feature reconstruction graph convolutional network for EEG emotion and cognition recognition,” *IEEE Trans. Neural Syst. Rehabil. Eng.*, vol. 31, pp. 3245–3254, 2023.

[19] W. Maass, “Networks of spiking neurons: the third generation of neural network models,” *Neural Netw.*, vol. 10, no. 9, pp. 1659–1671, 1997.

[20] J. Bartels, O. Gallou, H. Ito, et al., “An event-driven neural network for monitoring of epileptic seizures on low-power neuromorphic hardware,” *bioRxiv*, 2024.

[21] N. Kumar, G. Tang, R. Yoo, et al., “Decoding EEG with spiking neural networks on neuromorphic hardware,” *Trans. Mach. Learn. Res.*, 2022.

[22] W. Alzhrani, M. Doborjeh, Z. Doborjeh, et al., “Emotion recognition and understanding using EEG data in a brain-inspired spiking neural network architecture,” in *Proc. 2021 Int. Joint Conf. Neural Netw. (IJCNN)*, IEEE, 2021, pp. 1–9.

[23] B. Deng, Y. Fan, J. Wang, *et al.*, “Auditory perception architecture with spiking neural network and implementation on FPGA,” *Neural Networks*, vol. 165, pp. 31–42, 2023.

[24] M. A. Scrugli, G. Leone, P. Busia, *et al.*, “Real-time sEMG processing with spiking neural networks on a low-power 5K-LUT FPGA,” *IEEE Transactions on Biomedical Circuits and Systems*, 2024.

[25] K. Roy, A. Jaiswal, and P. Panda, “Towards spike-based machine intelligence with neuromorphic computing,” *Nature*, vol. 575, no. 7784, pp. 607–617, 2019. doi: 10.1038/s41586-019-1677-2

[26] A. L. Hodgkin and A. F. Huxley, “A quantitative description of membrane current and its application to conduction and excitation in nerve,” *The Journal of Physiology*, vol. 117, no. 4, pp. 500–544, 1952.

[27] A. Rast *et al.*, “Efficient learning in spiking neural networks,” *Neurocomputing*, p. 127962, 2024.

[28] E. M. Izhikevich, “Simple model of spiking neurons,” *IEEE Transactions on Neural Networks*, vol. 14, no. 6, pp. 1569–1572, 2003.

[29] W. Fang *et al.*, “SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence,” *Science Advances*, vol. 9, no. 40, p. eadi1480, 2023.

[30] A. Stanojevic, S. Woźniak, G. Bellec, *et al.*, “High-performance deep spiking neural networks with 0.3 spikes per neuron,” *Nature Communications*, vol. 15, no. 1, p. 6793, 2024.

[31] C. L. P. Chen, B. Chen, and T. Zhang, "AdamGraph: Adaptive Attention-Modulated Graph Network for EEG Emotion Recognition," IEEE Transactions on Cybernetics, 2025.

[32] W. Gerstner and W. M. Kistler, *Spiking Neuron Models: Single Neurons, Populations, Plasticity*, Cambridge, U.K.: Cambridge Univ. Press, 2002.

[33] B. B. Averbeck, P. E. Latham, and A. Pouget, "Neural correlations, population coding and computation," *Nature Reviews Neuroscience*, vol. 7, no. 5, pp. 358–366, 2006.

[34] C. Kayser, N. K. Logothetis, and S. Panzeri, "Millisecond encoding precision of auditory cortex neurons," *PNAS*, vol. 106, no. 38, pp. 15013–15018, 2009.

## 2. METHODOLOGY AND ALGORITHM

当前，可穿戴 EEG 情绪识别设备的主要挑战在于情绪识别系统在边缘设备上实现实时、高效、低功耗EEG信号处理时所面临的计算范式与信号特性不匹配。传统深度学习模型依赖密集且同步的矩阵运算，而SNN本质上表现为稀疏、事件驱动的神经活动[25]。这种差异导致现有模型难以在功耗和算力受限的设备上高效运行。为实现低功耗、低延迟的边缘情绪识别系统，需要一种既能充分捕获生物信号时空特征，又能在资源受限硬件上高效执行的新型计算架构。

针对上述问题，本文提出一种面向硬件优化的轻量级混合神经网络架构，通过在算法层面引入结构稀疏性与时间编码机制，以适配 FPGA 的并行计算与可自定义数据流的优点。该网络融合了高效的时空特征提取与脉冲时序计算能力，共由三个功能层级构成：Depthwise–Separable Gating Module（DSGM）、Time-to-First-Spike Encoding Module（TTFS Encoding Module）与Adaptive Temporal Spiking Neural Network（ATSNN）。

首先，DSGM 模块通过深度可分离卷积与通道–空间门控机制的协同作用，实现对脑电信号时空特征的分层建模与自适应增强。该模块由三个并行路径组成，分别对应深度可分离卷积、通道门控与空间门控分支。三条路径在结构上相互独立、在计算上可完全并行，不仅显著提升了 FPGA 的并行利用率与数据流效率，还能够根据输入特征的分布动态调整通道与空间权重，从而实现对关键脑区与显著模式的自适应强化。

其次，设计了一种基于 Time-to-First-Spike（TTFS） 的超低脉冲稀疏编码策略。该策略以在保持信息表达完整性的前提下实现最少脉冲发放与最低时延为核心目标，能够高效地将连续特征映射为事件驱动的脉冲序列。为适应不同特征分布，该编码机制引入动态范围自适应策略，同时实现了采用无除法的逻辑位移归一化方法，显著降低了归一化运算的复杂度与硬件实现开销。该模块实现了面向硬件的高效可部署脉冲编码，为 DSGM–SNN 异构体系提供了统一且低延迟的事件驱动接口。

最后，Adaptive Temporal Spiking Neural Network（ATSNN） 模块由多层基于 TTFS 编码 的脉冲神经网络构成。针对基于 TTFS 的 SNN 中固定时间窗导致的梯度不稳定与时间表达受限问题，本文提出一种基于活动依赖的在线自适应时间窗调整机制。该机制在训练过程中实时监测神经元脉冲发放特征，并依据乘性增益规则动态更新时间窗边界，从而实现层间时序的自适应调节。

整体而言，该混合架构在设计中融合了并行卷积、稀疏时间编码与事件驱动计算三类机制，既保持了对脑电信号复杂动态模式的建模能力，又具备良好的硬件映射友好性，为实时、低功耗的情绪识别系统提供了一种高效且可部署的解决方案。

### A.Neuron Model

脉冲神经网络（Spiking Neural Network, SNN）通过膜电位积累、阈值触发与事件驱动的计算机制，能够以更生物启发的方式表达神经活动的时序特征，因而在EEG等时间敏感型任务中表现出潜在优势。

在SNN的设计中，神经元模型是定义时域动态与信息编码方式的关键。当前常用的脉冲神经元模型主要包括 Hodgkin–Huxley (HH)[26] 、Spike Response Model (SRM)[27]、Izhikevich[28]以及Leaky Integrate-and-Fire (LIF) [29]。其中，HH 模型能够精确刻画神经元膜电位的生理动态，被视为生物神经计算的黄金标准，但其非线性微分结构导致计算复杂度极高，不适合在低功耗平台上部署。Izhikevich与SRM模型在生物真实性与计算效率之间取得平衡，但依然需要较高的资源开销。LIF模型由于结构简洁、参数较少、易于离散化计算，被广泛应用于工程实践中的脉冲神经网络设计。LIF 模型虽易于仿真，但其膜电位衰减需频繁乘法运算，在资源受限的 FPGA 或 SoC 上会显著增加功耗并限制并行效率。

<img src="%E8%AE%BA%E6%96%87.assets/image-20251028222808313.png" alt="image-20251028222808313" style="zoom:67%;" />

为解决LIF等模型在计算和硬件实现上的局限性，并充分发挥SNN的稀疏计算优势，本研究引入了 B1 脉冲神经元模型[30]，以实现更高的计算效率和硬件友好性。B1模型的核心就是引入了一个级联时间窗机制，对神经元的动态行为进行分段建模，将复杂的非线性膜电位演化过程简化为分段线性的形式。该机制有效避免了指数衰减与乘法运算等高硬件代价操作，显著降低了实现复杂度与功耗。

网络的第 $n$ 层的动态特性由时间窗 $[t_{\min}^{(n)}, t_{\max}^{(n)}]$ 划分为两个计算阶段。每一层的$t_{\min}^{(n)}$被定义为上一层的$t_{\max}^{(n-1)}$，即 $t_{min}^{(n)} =^{def} t_{max}^{(n-1)}$。
其膜电位演化可描述为：
$$
\epsilon \frac{dV_i^{(n)}(t)}{dt} =
\begin{cases}
\sum_j W_{ij}^{(n)} H(t - t_j^{(n-1)}), & t < t_{\min}^{(n)} \quad  \\
1, & t_{\min}^{(n)} \le t \le t_{\max}^{(n)} \quad 
\end{cases}
$$
其中，$\epsilon$ 为时间常数，$H(\cdot)$ 为亥维赛阶跃函数，$W_{ij}^{(n)}$ 为突触权重。

在 $t < t_{\min}^{(n)}$ 的时间区间内，神经元膜电位 $\frac{dV_i^{(n)}}{dt}$ 的斜率由加权输入脉冲 $\sum_j W_{ij}^{(n)} H(t - t_j^{(n-1)})$ 决定。随后，在 $t_{\min}^{(n)} \le t \le t_{\max}^{(n)}$ 的时间区间内，膜电位斜率 $\frac{dV_i^{(n)}}{dt}$ 被固定为1，以恒定的速率开始线性增长。

对于第 $n$ 层的神经元 $i$，其输出脉冲的时刻 $t_i^{(n)}$ 是基于上一层的输入 $t_j^{(n-1)}$ 和阈值 $\vartheta_i^{(n)}$计算得出的。由B1模型的膜电位动力学，可直接推导出该前向传播方程如式（2）所示：

$$
t_i^{(n)} = t_{\min}^{(n)} + \tau_c \vartheta_i^{(n)} - \sum_j W_{ij}^{(n)} \bigl( t_{\min}^{(n)} - t_j^{(n-1)} \bigr) \quad
$$

若神经元i的脉冲发放时间 $t_i^{(n)}$ 超过本层的时间窗$t_{\max}^{(n)}$，则被判定为静默神经元，不产生输出，从而进一步提升系统的稀疏性与能效。

### B. Layer Structure

在资源受限的轻量级 FPGA 平台上设计神经网络时，模型的计算复杂度、功耗预算与推理延迟与识别精度同样重要[14]。为在算法性能与硬件效率之间实现最优平衡，本文提出了一种面向硬件优化的轻量级混合神经网络架构，结合高效的时空特征提取与事件驱动的脉冲计算机制，以弥合算法结构与脑电信号特性的差距。==如图所示==，整个网络由三个功能层级构成。首先，采用深度可分离门控模块（Depthwise–Separable Gating Module, DSGM），通过将深度可分离卷积与通道–空间门控机制相结合，实现对 EEG 信号时空特征的高效建模。该模块在显著降低参数量与乘加运算复杂度的同时，仍能保持对脑区局部模式与跨通道相关性的表征能力。随后，事件编码层引入硬件友好的无除法 TTFS（Time-to-First-Spike）时间编码模块，将连续波形特征映射为稀疏脉冲时间序列，从而适配事件驱动的脉冲计算机制并减少冗余计算。最后，Adaptive Temporal Spiking Neural Module由基于 TTFS 编码的多层脉冲神经网络组成，利用脉冲发放的时间信息完成情绪状态分类，在保证竞争性识别精度的同时，实现低功耗与低延迟的推理性能。该混合架构在设计中充分考虑了 FPGA 与可穿戴设备的硬件约束，为实时、能效优化的脑电情绪识别提供了一种可行且可部署的解决方案。

1）DSGM(Depthwise–Separable Gating Module)

在脑电信号（EEG）分析与情绪识别等应用中，有效信息通常在通道与空间两个维度上高度分布式且非均匀。传统卷积操作在处理这类高维特征时存在计算效率低下与建模能力有限的矛盾[31]：一方面，普通卷积在通道维度上难以充分捕捉脑区间的依赖关系；另一方面，全局空间建模会带来大量参数和计算开销，不利于实时或嵌入式应用。为解决这一问题，我们提出了一种DSGM模块，在保持轻量化的同时对通道和空间维度进行自适应特征增强，主动地、动态地放大对情绪判断更关键的脑区信号和活动区域，==如图所示==。

该模块首先对输入特征图 $$X \in \mathbb{R}^{C \times H \times W}$$ 应用深度可分离卷积（Depthwise–Separable Convolution）。首先逐通道卷积用于提取每个通道内部的局部空间模式，能够捕获单脑区的细粒度活动信息。然后，逐点卷积实现跨通道的线性组合，从而高效融合来自不同脑区的特征信息。通过这种设计，模块在保证参数量极低的前提下，同时兼顾空间和通道特征建模能力，其计算复杂度相比标准卷积显著降低。
$$
F_{\mathrm{DW}} = f_{\mathrm{PW}}\big(f_{\mathrm{DW}}(X)\big)
$$
其中，$f_{\mathrm{DW}}(\cdot)$ 表示逐通道卷积操作；$f_{\mathrm{PW}}(\cdot)$表示逐点卷积操作（Pointwise Convolution）。

$F_{\mathrm{DW}}$ 作为 DSGM 模块的主干输出。与之并行地，DSGM 引入了通道-空间双路径门控机制以实现对特征的自适应调制从输入 $X$ 中提取通道门控权重 $G_{\mathrm{ch}}$ 与空间门控权重 $G_{\mathrm{sp}}$，用以加权主干特征，进一步提高网络对关键脑区和显著空间区域的响应能力。

通道门控通过全局平均池化获取每个通道的统计信息，并生成通道权重：
$$
\left\{
\begin{aligned}
s_c &= \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} X_{c,i,j}, \quad c=1,\dots,C,\\[1mm]
\sigma(x) &= \mathrm{clip}\Big(\frac{x}{8} + 0.5, 0, 1\Big),\\[1mm]
G_{\mathrm{ch}} &= \sigma\Big(f_{\mathrm{conv}}^{1\times1}(s) + b_c\Big)
\end{aligned}
\right.
$$
其中，$X_{c,i,j}$ 表示输入特征图 $X$ 在通道 $c$、空间位置 $(i,j)$ 的值，$H$ 和 $W$ 分别为特征图的高和宽，$C$ 为通道数；$s = [s_1,\dots,s_C]^T$ 表示各通道的全局平均值；$b_c$ 为 $1\times1$ 卷积的偏置；$\sigma(\cdot)$ 为硬件友好的 Hard-Sigmoid 激活函数，生成通道门控权重 $G_{\mathrm{ch}}$，用于对不同脑区的重要性进行建模，实现类脑的区域选择机制。

同时，空间门控在通道维度上计算平均值以获得空间显著性图$M$，并通过卷积与 Hard-Sigmoid 运算生成空间门控权重图$G_{\mathrm{sp}}$，可表示为：
$$
\left\{ 
\begin{aligned} 
M &= \frac{1}{C} \sum_{c=1}^{C} X_c,\\[2mm] 
G_{\mathrm{sp}} &= \sigma\Big(f_{\mathrm{conv}}^{3\times3}(M) + b_s\Big)
\end{aligned} 
\right.
$$
其中，$C$ 为总通道数，$X_c$ 是第 $c$ 个通道的特征图。$b_s$ 是 $3\times3$ 卷积的偏置。

空间门控能够突出局部活动显著的区域，使模型对关键脑区的瞬时活动更敏感，从而有效增强特征表达的判别能力。

最终，主路径特征与通道和空间门控权重通过逐元素乘积融合，并经过批量归一化（Batch Normalization）和 ReLU 激活得到模块输出：
$$
F_{\mathrm{out}} = f_{\mathrm{ReLU}}\Big(f_{\mathrm{BN}}\big(F_{\mathrm{DW}} \odot G_{\mathrm{ch}} \odot G_{\mathrm{sp}}\big)\Big)
$$
其中 $$\odot$$ 表示逐元素乘法操作。通过这种联合通道–空间调制策略，DSGM 不仅能够有效提取脑电信号中的关键信息，还保证了模型的轻量化和硬件友好性。

2）TTFS Encoding Module

SNN的低功耗潜力源于其事件驱动的计算范式，即只有在接收到脉冲时才进行计算。因此，一个高效的编码机制应当以最少的脉冲数量来承载最丰富的信息。相关工作虽然探索了多种编码方案，但如速率编码（Rate Coding）[32]、人口编码（Population Coding）[33]与相位编码（Phase Coding）[34]等，均因高延迟、高资源消耗或高实现复杂度，无法同时满足本系统对轻量化和实时性的严苛约束。

本研究采用的编码范式是首脉冲时间编码（Time-To-First-Spike, TTFS）。TTFS编码方式通过将更强的输入激转化为更早的脉冲发放时间，并且每个神经元在一个计算周期内最多只发放一个脉冲，实现了极致的脉冲稀疏性，确保了最低的理论功耗和最快的响应速度将DSGM模块提取的连续激活值转换为ATSNN所需的稀疏、离散的脉冲序列，是连接这两种计算范式的关键步骤。

然而，标准的 TTFS 编码方式依赖于浮点除法运算，其计算过程如下所示：  

$$
V_{norm} = \frac{V_{in} - V_{min}}{V_{max} - V_{min}}
$$

其中，$V_{in}$ 表示输入激活值，$V_{min}$ 与 $V_{max}$ 分别为激活值的最小与最大范围。由于浮点除法在硬件实现中代价较高，会显著增加逻辑单元的资源占用与功耗，本研究提出了一种基于动态范围自适应的无除法 TTFS 编码机制。该机制的核心思想是使用逻辑位移操作替代高成本的除法运算，从而在保持归一化效果的同时显著降低硬件复杂度。  

具体而言，系统在训练过程中动态跟踪激活值的范围 $(V_{max} - V_{min})$，并计算出一个大于该范围的最小二次幂 $S_p$作为新的归一化分母逻辑位移操作相当于对输入特征进行按位右移，从而以 $2^{-k}$ 的比例完成快速缩放。整个编码过程可描述如下：  

$$
\left\{
\begin{aligned}
S_p &= 2^{\lceil \log_2(V_{max} - V_{min}) \rceil} \\
V_{norm} &= f_{clip}\!\left(\frac{V_{in} - V_{min}}{S_p},\, 0,\, 1\right) \\
T_{spike} &= T_{max} - V_{norm} \cdot (T_{max} - T_{min})
\end{aligned}
\right.
$$
其中 $V_{in}$ 是来自DSGM的输入特征， $V_{min}$ 和 $V_{max}$ 是训练时跟踪的激活值动态范围， $S_p$ 是计算出的二次幂缩放因子，$f_{clip}$ 是钳位函数， $T_{spike}$ 是最终输出的脉冲时间。

3）ATSNN(Adaptive Temporal Spiking Neural Network)

在TTFS编码的SNN中，时间窗$$[T_{min}^{(n)}, T_{max}^{(n)}]$$是一个核心超参数，它直接定义了信息表示的分辨率。然而，在SNN训练期间，各层激活值的统计分布是动态变化的。采用一个静态、预设的时间窗会严重制约模型的学习与表达能力。更关键的是，静态窗口会导致梯度不稳定的风险。==如式（）所示==，隐藏层的梯度与前一层脉冲时间 $T_j^{(n-1)}$ 和时间窗下界 $T_{min}^{(n)}$ 的差值成线性关系。

$$\frac{\partial L}{\partial W_{ij}^{(n)}} = \frac{\partial L}{\partial T_i^{(n)}} \cdot \frac{\partial T_i^{(n)}}{\partial W_{ij}^{(n)}} = \frac{\partial L}{\partial T_i^{(n)}} \cdot (T_j^{(n-1)} - T_{min}^{(n)})$$

若时间窗范围设置过宽或过窄，梯度项 $(T_j^{(n-1)} - T_{min}^{(n)})$ 的分布将失衡，进而引发梯度爆炸或消失问题，造成训练震荡。因此，本研究引入一种基于神经活动依赖的在线时间窗自适应机制（Adaptive Temporal Window Adjustment），以在训练过程中动态重校准各层时间窗，从而提高训练稳定性与时域表达能力。  

ATSNN 模块基于 B1 型脉冲神经元构建为全连接结构。每一层 $n$ 的计算都在其专属的时间窗 $[T_{min}^{(n)}, T_{max}^{(n)}]$ 内进行，且相邻层的时间窗首尾级联，即 $T_{min}^{(n)} = T_{max}^{(n-1)}$。对于第 $n$ 层的神经元 $i$，其输出脉冲时刻 $T_i^{(n)}$ 由下式给出：  
$$
T_i^{(n)} = f_{clip}\left( \sum_j W_{ij}^{(n)} (T_j^{(n-1)} - T_{min}^{(n)}) + \vartheta_i^{(n)} + T_{min}^{(n)}, \ T_{min}^{(n)}, T_{max}^{(n)} \right)
$$
为了实现时间窗的动态调整，该机制在每次训练迭代中实时监控当前层 $n$ 的最早脉冲时间 $T_{earliest}^{(n)}$，并基于乘性增益规则计算新的时间窗上界 $T_{max,new}^{(n)}$：

$$
T_{max, new}^{(n)} = T_{max}^{(n)} + \lambda \cdot \left( (T_{earliest}^{(n)} - T_{min}^{(n)}) - \frac{T_{max}^{(n)} - T_{min}^{(n)}}{2} \right)
$$
其中 $$\lambda$$是控制速率的超参数。此调整后的$$T_{max, new}^{(n)}$$将立即被传递至下一层，作为其新的起始时间$$T_{min}^{(n+1)}$$。

该自适应机制将时间窗从一个静态的超参数转变为一个动态调整的变量，使网络能够适应训练过程中不断变化的激活值分布。其核心的乘性增益规则提供了一种双向自适应调整，它根据最早脉冲时间$$T_{earliest}^{(n)}$$与当前窗口中心$$\frac{T_{max}^{(n)} - T_{min}^{(n)}}{2}$$的偏离程度，来动态地缩放窗口大小。这种设计确保了脉冲始终在有效的时间分辨率内传播，从而避免了梯度不稳定问题，显著增强了SNN的训练稳定性和时域表达能力。

4）Output Layer

输出层被设计为非脉冲的积分单元，其职责是将在其时间窗口内接收来自ATSNN模块的所有脉冲进行积分。在窗口结束时，神经元 $i$ 内部累积的总电位值（Logit）$L_i$ 可得出：

$$L_i = \sum_j W_{ij}^{(L)} (T_{min}^{(L)} - T_j^{(L-1)}) + (T_{max}^{(L)} - T_{min}^{(L)} - \vartheta_i^{(L)})$$

此过程通过时域稀疏解码，将时间域信息转换回幅度域的决策值，选择Logit最大的神经元作为最终的分类。

### C. Network Quantization



## 3. HARDWARE ARCHITECTURE

本章详细阐述了所提出轻量级模型的硬件加速架构，旨在将算法层面的结构稀疏性与时间编码机制转化为 FPGA 平台上的实际能效与吞吐率优势。鉴于模型包含计算密集的同步卷积运算和离散的事件驱动脉冲处理，硬件设计面临着在统一架构内高效调度不同计算范式的挑战。本研究构建了一套基于流的混合流水线架构，通过定制化的微架构设计精确映射了从密集定点卷积计算到异步脉冲推理的完整处理链路。A 小节将首先从宏观层面概述基于全链路量化约束的整体数据通路与控制流，确立系统在资源受限条件下维持高吞吐率的设计支柱；B 小节将分析利用数据稀疏性进行动态功耗管理的事件驱动异步控制框架，重点阐述地址事件表达（AER）通信机制、握手协议与层级化门控如何共同最小化无效翻转；C 小节将深入剖析可重构张量引擎的资源复用机制、事件驱动计算单元的具体电路实现，展示系统如何通过底层硬件优化突破面积与性能的边界。

#### A. Overview of Hardware Architecture

本研究提出了一种面向边缘 EEG 推理的流式高能效异构处理架构。为了应对边缘设备严苛的功耗与面积限制，整体设计确立了以Q1.7全局量化策略与异步事件驱动思想为核心的基调，确保系统在极低资源预算下依然能够维持高精度的实时推理。如图 1 所示，在此基调下，系统由三项核心支柱支撑：1）覆盖全局的数据低位宽量化通路；2）可重构的张量流计算子系统3）以及基于 GALS 的跨域异步节能机制。上述原则共同确保架构在受限硬件资源下依然能够维持高吞吐率，同时充分利用生物信号的稀疏性实现能效提升。

为了展示各异构模块的协同工作机制，以下简述一次完整的端到端数据流处理过程：多通道 EEG 数据首先通过流式接口直接注入片上输入缓冲区。随后，处于同步时钟域的可重构张量计算引擎（TCE）启动工作，它首先被配置为标准卷积模式以提取底层特征；紧接着，TCE 动态重构其内部数据通路，并行处理主路径卷积与异构门控逻辑，并通过脉动级联调制完成特征融合。融合后的密集定点数据流随即进入无除法 TTFS 编码器，被高效转换为稀疏的时间脉冲序列，并封装为包含地址与绝对时间戳的 AER 数据包。这些事件包跨越时钟域边界，通过异步握手协议驱动后端的脉冲推理核心。在异步域内，SNN 引擎根据事件到达的节奏动态唤醒计算阵列，并行执行权重读取与膜电位累加。当最后一层神经元的电位更新完毕后，流式决策单元即时输出分类结果，完成一次完整的推理任务。

![image-20251119215221688](%E8%AE%BA%E6%96%87.assets/image-20251119215221688.png)

#### B. Event-Driven Asynchronous Control Framework

本研究在架构底层构建了一套层次化的事件驱动异步控制框架，旨在通过物理层面的信号握手与逻辑层面的资源调度，将计算活动严格限制在有效信息到达的瞬间。为了突破传统同步电路的时间刚性约束，系统采用了数据保持四相握手协议来协调同步卷积引擎与异步脉冲处理核心之间的交互。在宏观的跨时钟域通信上，这种机制摒弃了全局时钟树的统一步调，转而确立了一种“按需唤醒”的反应式工作模式：当无除法编码器完成一组脉冲的时间转换后，会拉高请求信号并将数据总线锁定，处于休眠状态的脉冲处理核心仅在接收到该请求的瞬间被硬件中断唤醒，完成事件处理后通过拉高应答信号闭合握手回路。这一设计确保了两个异构计算子系统仅在有效信息传递的纳秒级窗口内发生活动，而在数据稀疏的间隙自动回归零动态功耗的停滞状态。

作为片上神经信息传输的载体，地址事件表达（AER）接口被实现为一种基于广播-收集拓扑的同步总线，其核心逻辑体现了数据流驱动的异步特性。不同于传统的点对点路由，该架构中的AER发送端将包含8位量化时间戳与空间地址的事件包同时广播至所有计算阵列。为了在缺乏全局时钟对齐的环境下维持大规模并行计算的数据一致性，架构引入了分布式的完成脉冲收集器。该收集器利用同步计数器实时聚合来自并行处理单元的完成状态，仅当计数器值匹配当前激活的神经元数量时，才向主控逻辑反馈全局应答信号。这种设计在保证并行阵列严格同步的同时，有效避免了异步电路中因线路延迟差异导致的亚稳态风险。

为了进一步利用异步特性，模块内实施了层级化细粒度时钟门控策略，将异步的“完成即停止”思想引入到了同步流水线中。在卷积计算阶段，多路径融合模块采用基于握手信号的门控机制，并行启动主路径与门控路径的计算子模块，并在任一路径完成运算的当个时钟周期即刻切断其时钟网络，从而精确剔除了路径延迟差异导致的空闲翻转。而在脉冲计算阶段，SNN引擎则实施了层级循环折叠策略以适配脉冲神经网络逐层递减的规模特征。主控FSM根据当前计算层的神经元规模动态调整活跃阵列的窗口大小：在处理64神经元的首层时，系统唤醒全部16个计算阵列；而在处理随后的32神经元与3神经元层时，分别仅维持前8个与前1个阵列的时钟翻转，其余硬件资源被强制处于深度休眠状态。这种从握手协议到逻辑门控的全链路协同设计，确保了硬件加速器的能耗曲线能够实时且精确地跟随EEG信号的时变稀疏性。

### C. Hardware Mechanism and Optimizations

为了在边缘端受限的硅片面积内实现复杂的混合计算逻辑，微架构设计遵循“极致复用”与“按需激活”的原则。本节将深入剖析前端同步卷积引擎的动态调度机制、中间层无除法时间编码接口的转换逻辑，以及后端事件驱动脉冲计算单元的电路级实现，展示系统如何通过底层硬件优化突破面积与性能的边界。

1) Reconfigurable Tensor Compute Engine

为了在受限的硬件面积下实现复杂的计算逻辑，前端架构的核心是一个基于流的可重构张量计算引擎。该引擎的设计宗旨并非简单地堆叠算力，而是通过时分复用同一组物理资源来支撑不同的卷积计算范式。该模块主要由逻辑控制单元、共享 MAC 阵列、加法树以及多功能激活模块构成。为了维持卷积操作的高吞吐率，引擎前端集成了基于移位寄存器的行缓存，该结构利用数据局部性原理实现了 $3 \times 3$ 窗口的滑动更新，确保了计算阵列的高吞吐量。

在初始特征提取阶段，逻辑控制单元将 TCE 配置为标准卷积模式。数据流经由行缓存 (Line Buffer) 对齐后，送入共享的 MAC 阵列 (MAC Arrays) 与加法树 (Add Tree) 完成深度可分离卷积运算，结果经激活模块 (Activation Module) 处理后存入输出缓冲区 (Output Buffer)。随后进入多分支门控处理阶段，逻辑控制单元对 TCE 内部的数据通路进行动态重定向，以实现资源的异构复用。此时，同一组 MAC 阵列与加法树被重新配置以分别执行主路径的深度卷积与通道门控的点对点卷积。在此期间，激活模块也相应调整工作模式，针对主路径数据，架构引入了融合式后处理单元以高效执行批量归一化与 ReLU 激活。在张量引擎的输出级，为了进一步压缩流水线延迟并降低资源消耗，架构引入了针对特定激活函数的硬件融合优化单元。如图(a)所示，批量归一化（BatchNorm）与ReLU激活被整合为一个紧凑的后处理流水线。利用卷积与归一化操作的线性特性，我们将BN层的缩放与偏移参数在离线阶段预先折叠至卷积偏置中，从而将硬件层面的归一化简化为累加器输出端的单次加法操作。紧随其后的是重量化逻辑，通过算术右移器将高精度数据动态缩放回低位宽定点域。ReLU激活逻辑被实现为基于最高有效位（MSB）的选择电路：MSB信号直接驱动多路复用器，当检测到负值时输出置零，否则直通数据，从而在单时钟周期内完成了归一化、量化与非线性激活。

此外，针对门控路径所需的Hard-Sigmoid函数，设计采用了如图(b)所示的“移位-加法-钳位”电路以规避昂贵的指数运算。输入数据首先通过硬连线的右移3位操作实现乘0.125的缩放，随后加上对应0.5的定点常数。并行比较器实时监测运算结果是否超出[0, 1]的定点数值范围，并生成控制信号驱动多路复用器进行边界钳位，这种纯组合逻辑实现极大地减少了门控分支的面积开销。

![image-20251119213848340](%E8%AE%BA%E6%96%87.assets/image-20251119213848340.png)

2) Division-Free Time Encoding Module

作为连接同步数值计算域与异步事件处理域的关键枢纽，无除法时间编码模块旨在消除传统时间编码算法中浮点除法所引入的显著硬件开销。该模块在微架构层面实施了“二次幂缩放”的算术优化策略，通过部署并行算术移位器阵列取代除法器，在单时钟周期内高效地将 Q1.7 格式的输入特征映射为脉冲延迟，其变换逻辑严格遵循 $t = T_{max} - ((x - K_{min}) \gg K_{shift})$ 的线性范式。紧接着，序列化电路将并行生成的时间戳转换为串行的地址事件表达（AER）数据包。为了进一步提升ATSNN的处理效率，编码器内部集成了硬件级的阈值过滤机制，能够实时抑制超出最大时间窗口的无效事件。该模块不仅完成了数据表征的转换，还通过基于握手的异步交互协议确立了跨时钟域的弹性边界，从而确保了数据流在异构传输路径上的完整性与鲁棒性。

![image-20251119214212567](%E8%AE%BA%E6%96%87.assets/image-20251119214212567.png)

3) Event-Driven SNN Compute Engine

SNN 计算引擎围绕 Q1.7 量化协议构建，其微架构设计的核心在于将计算资源与片上存储进行阵列化集成。为了解决脉冲神经网络中权重访问密集的瓶颈，计算逻辑被划分为 16 个独立的 SNN 计算阵列 (SNN Compute Arrays)，每个阵列内部紧密集成了 4 个并行工作的 异步微处理核 (Asynchronous Micro-Processing Kernels)。与之相匹配，架构采用了阵列级宽字存储方案：每个计算阵列均配备了一个专属的 32 位宽接口物理存储体。在单次读取周期内，该存储体输出的 32 位数据包在阵列内部被即时解包为 4 个 8 位突触权重，并同步分发给 4 个微处理核，从而在低频时钟下实现了等效于高带宽总线的权重吞吐率，有效消除了计算阵列的访存等待。

这些微处理核的电路实现完全遵循异步响应范式，其核心在于每个内核内部集成的独立微型状态机能够对事件请求做出瞬时响应。在无任务的空闲状态下，内核的时钟网络被门控机制强制关闭以消除动态功耗，而一旦检测到有效的事件请求，状态机即刻唤醒并流水化执行时间戳锁存、权重读取以及膜电位累加等核心操作。为了协调阵列内部多个并行处理节点的时序，每个计算阵列集成了一个基于计数器的收集器模块。该模块负责实时追踪并聚合内部四个微处理核的完成状态，确保只有当所有被激活的节点均完成计算任务后才向全局总线发送统一的应答信号，这种设计在严格保证并行计算数据一致性的同时显著降低了握手通信的控制开销。

作为连接稀疏数据流与并行计算阵列的通信枢纽，地址事件表达（AER）接口模块在硬件底层实现了一种基于多路复用的广播架构。该接口配置有双向数据通路，通过有限状态机调度在外部输入流与内部递归流之间动态切换：当处理输入层时，模块接收来自前端编码器的串行事件包；而在处理隐藏层时，则启用集成的层间缓冲与重编码逻辑，将上一层的累加电位实时转换为新的脉冲事件序列并回环至输入总线。在传输层面，接口采用广播拓扑将 AER 数据包同步分发至所有启用的微处理核，确保了全连接层中的权重共享与并行更新。当最后一层神经元的膜电位更新完毕后，结果被送入流式决策单元，该单元在接收 32 位电位数据的同时实时比较并锁存最大值索引，实现了零延迟的分类结果读出。此外，针对 SNN 层级规模逐层递减的特性，主控逻辑实施了层级循环折叠策略，使动态功耗严格随网络规模线性下降。





## 4. IMPLEMENTATION AND RESULTS

A. Experimental Details

1）Datasets

为保证与既往研究的可比性，本研究仅采用公开的**EEG**数据集作为评估基准：SEED 系列（SEED、SEED-IV、SEED-V）、DEAP 与 DREAMER。SEED 由上海交通大学 BCMI 团队采集，包含 15 名受试者的 62 通道 EEG，三次会话记录，刺激为若干电影片段（单段约 4 分钟），常见预处理包括下采样至 200 Hz、带通滤波及频域特征（如差分熵）提取（Zheng et al., 2016）。SEED-IV 为 SEED 在情感类别上的四类扩展，保留 62 通道与多会话设计，常用于四分类任务验证；SEED-V 则在被试规模与情感细粒度上进一步扩展并提供预计算的频域特征，便于更细致的情感分类比较。DEAP 包括从 32 名受试者（17 名男性、15名女性）收集的 32 通道脑电图数据。受试者的情绪是通过一分钟长的音乐视频剪辑来诱导的。每次刺激后，受试者都会根据 Russell （1980） 提出的二维情绪量表对他们的情绪体验进行评分。DREAMER 采用便携设备采集约 23 名被试的 14 通道 EEG（采样 128 Hz），因其轻量 EEG 配置常被用于轻量化模型的性能验证。为保证比较公平性，本研究在所有数据集上仅使用 EEG 通道，统一采用各自通用的预处理约定，随后将处理结果用于后续特征提取与分类评估。

2） Data preprocessing

本研究的数据预处理流程旨在从原始的多通道脑电（EEG）信号中提取具有判别性的时空特征，并对其进行平滑处理以增强信号的稳定性。整个流程主要包括基线校正、数据分段、特征提取、空间映射和时间动态平滑，具体步骤如下。

首先，我们对原始的多通道脑电信号进行基线校正。采用Z-Score标准化方法，利用每个试次（trial）开始前一段时间的静息态数据作为基线。具体而言，我们计算基线时段内每个通道信号的平均值和标准差，并利用这两个统计量对整个试次的信号进行标准化处理。该步骤有效地移除了通道间的基线漂移，并减少了不同试次间的信号差异。

完成基线校正后，我们将连续的脑电信号分割成固定长度的样本。我们使用一个固定长度的非重叠滑动窗口对每个试次的数据进行分割。为了更好地捕捉信号的动态变化，每个样本被进一步分解为多个连续的、更短的时间帧，形成一个维度为 T × C × S 的张量，其中T代表时间帧的数量，C为通道数，S为每个时间帧的采样点数。

接下来是特征提取阶段。针对每个时间帧，我们计算其功率谱熵（Power Spectrum Entropy, PSE）作为核心特征。功率谱熵能够有效衡量信号在频域中的不确定性或复杂性。通过该计算，T × C × S 的原始信号样本被转换为一个 T × C 的特征矩阵。
$$
H_{\mathrm{PSE}} = -\sum_{i=1}^{S} p_i \ln(p_i), \quad 
p_i = \frac{P(f_i)}{\sum_{j=1}^{S} P(f_j)}
$$

其中：$P(f_i)$是信号在频率$f_i$处的功率谱密度；$p_i$是归一化后的功率分布；$N$是频率点数；$ln$是自然对数。

为了保留并利用电极在头皮上的空间拓扑信息，我们将提取的多通道特征映射到一个二维网格中。该映射关系根据标准电极系统的物理位置来确定，使得空间上相邻的电极在特征图上也保持邻近。经过此步骤，每个样本被表示为多个连续的二维特征图，构成了一个具有时空结构的特征序列。

最后，为了对特征序列进行平滑处理并建模其内在的时间动态性，我们引入了线性动力学系统（Linear Dynamical System, LDS）对每个试次内的特征序列进行时间上的平滑。

|          | Sample Shape |         |                |
| -------- | ------------ | ------- | -------------- |
| Datasets | SIZE         | CHANNLE | Sliding window |
| SEED     | 8*9          | 4       | 1s             |
| SEED IV  | 8*9          | 4       | 1s             |
| SEED V   | 8*9          | 4       | 1s             |
| DEAP     | 6*7          | 6       | 1.5s           |
| DREAMER  | 4*5          | 9       | 1.5s           |

 

### B. Experimental Settings:

整个实验环境基于如下计算机配置：12 核 vCPU Intel(R) Xeon(R) Platinum 8255C 处理器（主频 2.50 GHz）与 NVIDIA GeForce RTX 2080 Ti（11 GB 显存）GPU，运行在 Ubuntu 22.04 系统下的 Python 3.12 环境中，使用 PyTorch 2.3.0 与 CUDA 12.1。
对于机器学习模型，我们参考了以往研究的经验并尝试寻找最优参数；对于深度学习模型，采用 Adamw优化器 来最小化交叉熵损失函数，学习率设为1e-4，batch size为 8，epoch设为 300，并使用早停防止过拟合。

随后，我们采用多种指标对所提出的 LSNNet 模型进行客观评估，包括 准确率（Acc）、特异性（Spe）、敏感性（Sen）、精确率（Pre） 以及 F1-score（F1） 。实验结果以平均值（mean） 和 标准差（Std） 的形式给出。

$\begin{matrix}Acc\mathrm{=}\frac{TP\mathrm{+}TN}{TP\mathrm{+}TN\mathrm{+}FP\mathrm{+}FN}\end{matrix}$

$\begin{matrix}Spe\mathrm{=}\frac{TN}{TN\mathrm{+}FP}\end{matrix}$

$\begin{matrix}Sen\mathrm{=}\frac{TP}{TP\mathrm{+}FN}\end{matrix}$

$\begin{matrix}Pre\mathrm{=}\frac{TP}{TP\mathrm{+}FP}\end{matrix}$

$\begin{matrix}F\mathrm{1=}\frac{\mathrm{2×}Pre\mathrm{×}Sen}{Pre\mathrm{+}Sen}\end{matrix}$

其中，$(TP)\mathrm{、}(TN)\mathrm{、}(FP)\mathrm{、}(FN)$分别表示真正例（True Positive）、真反例（True Negative）、假正例（False Positive）和假反例（False Negative)。

此外，为了评估模型的计算效率，我们采用 浮点运算量（FLOPs） 和 乘加运算量（Mult-Adds） 来衡量模型的整体计算复杂度。最后，将训练好的模型进行INT8量化后部署在 Xilinx Zynq-7000 SoC (XC7Z020-2CLG400) 平台上，以评估其硬件资源占用情况，其中 LUT、LUTRAM、FF 与BRAM 被用于测量模型在硬件上的最小存储资源需求。

### C. Classification Methods Used for Comparison

### D. Classification Results

| SEED              | Acc    | Spe    | Sen    | Pre    | F1     |
| ----------------- | ------ | ------ | ------ | ------ | ------ |
| RF                | 0.9191 | 0.9330 | 0.9180 | 0.9150 | 0.9184 |
| k-NN              | 0.9082 | 0.9212 | 0.9052 | 0.8928 | 0.9021 |
| XGBoost           | 0.8811 | 0.9072 | 0.8681 | 0.8639 | 0.8712 |
| SqueezeNet        | 0.9404 | 0.9658 | 0.9337 | 0.9325 | 0.9513 |
| MobileNetV3-Small | 0.9340 | 0.9423 | 0.9116 | 0.8915 | 0.9209 |
| MobileNetV3-Large | 0.9085 | 0.9266 | 0.9089 | 0.9264 | 0.8878 |
| ShuffleNetV2      | 0.9101 | 0.9551 | 0.9294 | 0.9192 | 0.8908 |
| EfficientNet-B0   | 0.9535 | 0.9767 | 0.9440 | 0.9479 | 0.9562 |
| EfficientNet-B3   | 0.9603 | 0.9802 | 0.9496 | 0.9619 | 0.9558 |
| Deep ConvNet      | 0.9588 | 0.9793 | 0.9457 | 0.9525 | 0.9642 |
| Shallow ConvNet   | 0.9227 | 0.9615 | 0.9311 | 0.9529 | 0.9496 |
| EEGNet            | 0.9134 | 0.9567 | 0.9097 | 0.9282 | 0.9041 |
| EESCN             | 0.9608 | 0.9804 | 0.9647 | 0.9329 | 0.9707 |
| DH-SNN            | 0.9654 | 0.9840 | 0.9507 | 0.9601 | 0.9575 |
| ours              |        |        |        |        |        |

| SEEIV             | Acc    | Spe    | Sen    | Pre    | F1     |
| ----------------- | ------ | ------ | ------ | ------ | ------ |
| RF                | 0.8985 | 0.9230 | 0.8769 | 0.8803 | 0.8923 |
| k-NN              | 0.8905 | 0.9226 | 0.8763 | 0.8780 | 0.8868 |
| XGBoost           | 0.8627 | 0.9135 | 0.8602 | 0.8473 | 0.8637 |
| SqueezeNet        | 0.9235 | 0.9744 | 0.9200 | 0.9327 | 0.9220 |
| MobileNetV3-Small | 0.9261 | 0.9751 | 0.9246 | 0.9459 | 0.9339 |
| MobileNetV3-Large | 0.9034 | 0.9487 | 0.8962 | 0.8962 | 0.9096 |
| ShuffleNetV2      | 0.9371 | 0.9791 | 0.9179 | 0.9345 | 0.9228 |
| EfficientNet-B0   | 0.9216 | 0.9734 | 0.9196 | 0.9272 | 0.9225 |
| EfficientNet-B3   | 0.9296 | 0.9760 | 0.9307 | 0.9409 | 0.9309 |
| Deep ConvNet      | 0.9248 | 0.9746 | 0.9127 | 0.9288 | 0.9313 |
| Shallow ConvNet   | 0.8949 | 0.9612 | 0.9194 | 0.8875 | 0.8951 |
| EEGNet            | 0.8803 | 0.9099 | 0.8196 | 0.8503 | 0.8780 |
| EESCN             | 0.9260 | 0.9715 | 0.9334 | 0.9177 | 0.9351 |
| DH-SNN            | 0.9308 | 0.9827 | 0.9295 | 0.8996 | 0.9258 |
| ours              |        |        |        |        |        |

| SEEDV             | Acc    | Spe    | Sen    | Pre    | F1     |
| ----------------- | ------ | ------ | ------ | ------ | ------ |
| RF                | 0.8923 | 0.9198 | 0.8745 | 0.8820 | 0.8782 |
| k-NN              | 0.8791 | 0.9127 | 0.8573 | 0.8680 | 0.8724 |
| XGBoost           | 0.8841 | 0.9230 | 0.8581 | 0.8895 | 0.8520 |
| SqueezeNet        | 0.9037 | 0.9768 | 0.9143 | 0.9115 | 0.9129 |
| MobileNetV3-Small | 0.9191 | 0.9466 | 0.9213 | 0.9296 | 0.9166 |
| MobileNetV3-Large | 0.8973 | 0.8894 | 0.8653 | 0.8945 | 0.8836 |
| ShuffleNetV2      | 0.9022 | 0.9547 | 0.8721 | 0.9188 | 0.9122 |
| EfficientNet-B0   | 0.9165 | 0.9758 | 0.9080 | 0.9138 | 0.9334 |
| EfficientNet-B3   | 0.9119 | 0.9696 | 0.9068 | 0.9249 | 0.9135 |
| Deep ConvNet      | 0.9051 | 0.9763 | 0.8972 | 0.9124 | 0.8958 |
| Shallow ConvNet   | 0.8578 | 0.9620 | 0.8447 | 0.8672 | 0.8769 |
| EEGNet            | 0.8608 | 0.9389 | 0.8512 | 0.8717 | 0.8565 |
| EESCN             | 0.8901 | 0.9591 | 0.8986 | 0.8510 | 0.9015 |
| DH-SNN            | 0.9151 | 0.9716 | 0.9147 | 0.9418 | 0.9442 |
| ours              |        |        |        |        |        |

| DREAMER           | Acc    | Spe    | Sen    | Pre    | F1     |
| ----------------- | ------ | ------ | ------ | ------ | ------ |
| RF                | 0.8614 | 0.9222 | 0.8218 | 0.8052 | 0.8561 |
| k-NN              | 0.8888 | 0.9189 | 0.8815 | 0.8789 | 0.8802 |
| XGBoost           | 0.8556 | 0.8995 | 0.8456 | 0.8683 | 0.8546 |
| SqueezeNet        | 0.9108 | 0.9664 | 0.9003 | 0.9032 | 0.9017 |
| MobileNetV3-Small | 0.8924 | 0.9629 | 0.8878 | 0.8965 | 0.8823 |
| MobileNetV3-Large | 0.9163 | 0.9762 | 0.9258 | 0.9020 | 0.9119 |
| ShuffleNetV2      | 0.9285 | 0.9749 | 0.9237 | 0.9340 | 0.9215 |
| EfficientNet-B0   | 0.8835 | 0.9404 | 0.8909 | 0.8603 | 0.8950 |
| EfficientNet-B3   | 0.8554 | 0.9256 | 0.8921 | 0.8316 | 0.9042 |
| Deep ConvNet      | 0.8981 | 0.9495 | 0.9129 | 0.8978 | 0.9143 |
| Shallow ConvNet   | 0.8361 | 0.8628 | 0.8128 | 0.8438 | 0.8787 |
| EEGNet            | 0.8645 | 0.8819 | 0.8549 | 0.8646 | 0.8977 |
| EESCN             | 0.9232 | 0.9727 | 0.9128 | 0.9241 | 0.9559 |
| DH-SNN            | 0.8989 | 0.9659 | 0.8803 | 0.9152 | 0.9078 |
| ours              |        |        |        |        |        |

| DEAP              | Acc    | Spe    | Sen    | Pre    | F1     |
| ----------------- | ------ | ------ | ------ | ------ | ------ |
| RF                | 0.7943 | 0.9258 | 0.7002 | 0.8383 | 0.7883 |
| k-NN              | 0.8275 | 0.9020 | 0.8096 | 0.7878 | 0.8059 |
| XGBoost           | 0.7526 | 0.9121 | 0.7315 | 0.7794 | 0.7467 |
| SqueezeNet        | 0.6774 | 0.8538 | 0.7538 | 0.7076 | 0.6922 |
| MobileNetV3-Small | 0.8411 | 0.9453 | 0.8360 | 0.8225 | 0.8050 |
| MobileNetV3-Large | 0.8629 | 0.9217 | 0.8149 | 0.8372 | 0.7963 |
| ShuffleNetV2      | 0.7721 | 0.9202 | 0.7586 | 0.7852 | 0.7686 |
| EfficientNet-B0   | 0.8392 | 0.9443 | 0.8311 | 0.8471 | 0.8370 |
| EfficientNet-B3   | 0.7014 | 0.8935 | 0.7530 | 0.7030 | 0.6815 |
| Deep ConvNet      | 0.8219 | 0.8999 | 0.8867 | 0.7998 | 0.7897 |
| Shallow ConvNet   | 0.8058 | 0.8979 | 0.7749 | 0.8236 | 0.7953 |
| EEGNet            | 0.8175 | 0.9102 | 0.7850 | 0.8194 | 0.8319 |
| EESCN             | 0.8765 | 0.9482 | 0.8594 | 0.8717 | 0.8655 |
| DH-SNN            | 0.7575 | 0.8885 | 0.7689 | 0.7963 | 0.7768 |
| ours              |        |        |        |        |        |

### D. ASIC Implementation

### 5.2 模型性能分析

软件：不同模型架构的性能对比：准确率、精确率、召回率、F1分数等指标
混淆矩阵：各类情绪分类的预测结果
鲁棒性分析：加入不同强度的高斯噪声，模拟实际工作环境

**后端消融对比**

- **目的：** 证明SNN后端 相较于传统ANN后端在能效上的巨大优势。
- **设计：** 保持前端（`DepthwiseSeparableConv`）完全一致。构建一个基线模型，将其后端的 `SpikingDense` 层 替换为等效拓扑的 `nn.Linear` 层。在相同条件下训练后，对比二者的**验证精度**（应相近）与**计算成本**（SNN的平均脉冲数 vs. ANN的MACs）。

**前端消融对比**

- **目的：** 证明深度可分离卷积（DSC）前端 相较于传统卷积在计算成本上的优越性。
- **设计：** 保持后端（`SpikingDense`）完全一致。构建一个对照组模型，将其前端的 `DepthwiseSeparableConv` 替换为 `nn.Conv2d`（标准卷积）。对比二者的**验证精度**（应相近）以及前端的**参数量**和**MACs**。

### 5.3 硬件性能分析
FPGA的实际准确率
吞吐量
FPGA资源占用
量化分析：从32、16位、8位、6位到4位

## 6. CONCLUSION
### 6.1 研究总结
### 6.2 未来工作展望
